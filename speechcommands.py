# -*- coding: utf-8 -*-
"""SpeechCommands.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1A3rClBLkP8Ebq9_Jn9vfRrZsOW2A64PQ
"""

from google.colab import drive
drive.mount("/content/gdrive")

import json
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow.keras as keras
from keras.models import Sequential
from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization
from tensorflow.keras import layers

DATA_PATH = "/content/gdrive/MyDrive/Ochagavia_TrabajoFinal_RedesNeuronales/SpeechCommands_data.json"
LEARNING_RATE = 0.0001
NUM_KEYWORDS = 10

def load_dataset(data_path):
  with open(data_path, "r") as fp:
    data = json.load(fp)

  #Obtener los 'features' y las etiquetas
  X = np.array( data["MFCCs"] )
  y = np.array( data["labels"] )
  
  return X,y

def get_data_splits(data_path, test_size=0.1, validation_size =0.1):
  #Cargar el dataset
  X, y = load_dataset(data_path)

  #Crear las divisiones del dataset en entrenamiento/validación/testeo
  X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = test_size)
  X_train, X_validation, y_train, y_validation = train_test_split(X_train,y_train, test_size = validation_size)
  
  #Ajustar el dimensión de las entrada de la red de arreglos 2D a arreglos 3D
  X_train = X_train[..., np.newaxis]
  X_validation = X_validation[..., np.newaxis] 
  X_test = X_test[..., np.newaxis]
  
  return X_train, X_validation, X_test, y_train, y_validation, y_test

def build_model(input_shape, learning_rate, error = "sparse_categorical_crossentropy"):
  #Construcción del modelo
  model = Sequential()

  #1ra capa convolucional
  model.add( Conv2D(64, (3,3), activation="relu", 
                    input_shape = input_shape))
  model.add( BatchNormalization() )
  model.add( MaxPool2D((3,3), strides = (2,2), 
                       padding = "same") )

  #2da capa convolucional
  model.add( Conv2D(32, (3,3), activation="relu"))
  model.add( BatchNormalization() )
  model.add( MaxPool2D((3,3), strides = (2,2), 
                       padding = "same") )
  
  #3ra capa convolucional
  model.add( Conv2D(32, (2,2), activation="relu"))
  model.add( BatchNormalization() )
  model.add( MaxPool2D((2,2), strides = (2,2), 
                       padding = "same") )

  #'Flatten' -> pasa la salida a un arreglo unidimensional
  #             para 'alimentar' una 'dense layer'
  model.add( Flatten() )
  model.add( Dense(64, activation="relu") )
  model.add( Dropout(0.3) )

  #Clasificador con función de activación 'softmax' 
  model.add( Dense(NUM_KEYWORDS, activation="softmax") )

  #Compilación del modelo
  optimiser = keras.optimizers.Adam(learning_rate=learning_rate)
  model.compile(optimizer = optimiser,
                loss = error,
                metrics = ["accuracy"])

  return model

def main(epoch, batch_size):
  with open(DATA_PATH, "r") as fp:
    data = json.load(fp)
  
  #Carga de 'features' en los train/validation/test data splits
  X_train, X_validation, X_test, y_train, y_validation, y_test = get_data_splits(DATA_PATH)
 
  #Construcción del modelo
  input_shape = (X_train.shape[1],X_train.shape[2],X_train.shape[3]) #(#segments, #coefficients = 13, 1)
  model = build_model(input_shape, LEARNING_RATE)

  #Entrenamiento de la red
  hist = model.fit(X_train, y_train,
            epochs = EPOCHS,
            batch_size = BATCH_SIZE,
            validation_data = (X_validation, y_validation)
            )

  #Evaluación de la red
  test_error, test_accuracy = model.evaluate(X_test, y_test)
  print(f"Test error: {test_error}, test accuracy: {test_accuracy}")

  #Gráficos para evaluar el comportamiento de la red
  print(f'Epoch: {epoch}, Batch size: {batch_size}')
  #plt.figure(0)
  #plt.plot(hist.history['accuracy'],'o')
  #plt.plot(hist.history['val_accuracy'],'o')
  #plt.title('Network Accuracy')
  #plt.ylabel('Accuracy')
  #plt.xlabel('Epoch')
  #plt.legend(['Train','Validation'],loc= 'upper left')
  #plt.grid()
  #plt.show()

  #plt.figure(1)
  #plt.plot(hist.history['loss'],'o')
  #plt.plot(hist.history['val_loss'],'o')
  #plt.title('Network Loss')
  #plt.ylabel('Loss')
  #plt.xlabel('Epoch')
  #plt.legend(['Train','Validation'],loc= 'upper right')
  #plt.grid()
  #plt.show()  

  fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))
  axes[0].plot(hist.history['accuracy'],'o')
  axes[0].plot(hist.history['val_accuracy'],'o')
  axes[0].grid()
  axes[0].set(title='Network Accuracy')
  axes[0].set(ylabel='Accuracy')
  axes[0].set(xlabel='Epoch')
  axes[0].legend(['Train','Validation'],loc= 'lower right')

  axes[1].plot(hist.history['loss'],'o')
  axes[1].plot(hist.history['val_loss'],'o')
  axes[1].grid()
  axes[1].set(title='Network Loss')
  axes[1].set(ylabel='Loss')
  axes[1].set(xlabel='Epoch')
  axes[1].legend(['Train','Validation'],loc= 'upper right')
  fig.tight_layout()

  return model, hist, X_test, y_test

EPOCHS = 5
BATCH_SIZE = 128
if __name__ == "__main__":
  model, hist, X_test, y_test = main(EPOCHS, BATCH_SIZE)

EPOCHS = 10
BATCH_SIZE = 64
if __name__ == "__main__":
  model, hist, X_test, y_test = main(EPOCHS, BATCH_SIZE)

"""La validation accuracy bajó, pero la validation loss siguió bajando"""

EPOCHS = 20
BATCH_SIZE = 64
if __name__ == "__main__":
  model, hist, X_test, y_test = main(EPOCHS, BATCH_SIZE)

EPOCHS = 30
BATCH_SIZE = 64
if __name__ == "__main__":
  model, hist, X_test, y_test = main(EPOCHS, BATCH_SIZE)

EPOCHS = 40
BATCH_SIZE = 64
if __name__ == "__main__":
  model, hist, X_test, y_test = main(EPOCHS, BATCH_SIZE)

EPOCHS = 40
BATCH_SIZE = 32
if __name__ == "__main__": 
  model, hist, X_test, y_test = main(EPOCHS, BATCH_SIZE)

EPOCHS = 55
BATCH_SIZE = 16
if __name__ == "__main__": 
  model, hist, X_test, y_test = main(EPOCHS, BATCH_SIZE)

#Predictions
with open(DATA_PATH, "r") as fp:
    data = json.load(fp)
    
predictions = model.predict(X_test)
p = np.zeros((len(predictions),predictions.shape[1]))

for k in range(len(predictions)):
  list_index = [0,1,2,3,4,5,6,7,8,9]
  x = predictions[k]
  for i in range(10):
    for j in range(10):
      if x[list_index[i]] > x[list_index[j]]:
        temp = list_index[i]
        list_index[i] = list_index[j]
        list_index[j] = temp
  p[k] = list_index
p = p.astype(int)

for k in range(15):
  print(k)
  print('Clase original: ', np.array(data["mappings"])[y_test[k]])
  print('Predicción: ', np.array(data["mappings"])[p[k][0]],':',round(predictions[k][p[k][0]]*100,4), '%')